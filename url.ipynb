{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering intern test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "### webscrape all the sites in text.csv and extract only article title and article content\n",
    "### save them all to a text file (URL_ID.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "def facile(link):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import numpy as np\n",
    "\n",
    "    #the headers dictionary lets the site know the request is coming from a browser \n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "    }\n",
    "\n",
    "    r = requests.get(link, headers=headers)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    article_data = soup.find(\"title\").text \n",
    "\n",
    "    article_content = soup.find(\"div\", class_=\"td-post-content\").text\n",
    "\n",
    "    URL_ID = article_data.ljust(len(article_data)+100) + article_content\n",
    "    #create a new text file before the code below\n",
    "    text_file = open(\"URL_ID.txt\", \"a\",encoding=\"utf-8\")\n",
    "    n = text_file.write(URL_ID)\n",
    "    text_file.close()\n",
    "        \n",
    "for i in test.URL:\n",
    "    facile(i)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "### webscrape all the sites and extract specific calculated data\n",
    "### save them all to a dataframe and save as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"Input your csv file containing list of sites with same html and css structure.csv\")\n",
    "\n",
    "\n",
    "#the column with the site link is URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "seee= pd.read_csv('Output Data Structure.csv')\n",
    "# the above csv file contains the headers of the expected output data\n",
    "\n",
    "#the function 'easy' goes to a given link and extracts the article's content and extracts specific calulated data such as POSITIVE SCORE','NEGATIVE SCORE' \n",
    "                       # 'POLARITY SCORE'\n",
    "                       #'SUBJECTIVITY SCORE'\n",
    "                       #'AVG SENTENCE LENGTH'\n",
    "                      # 'PERCENTAGE OF COMPLEX WORDS'\n",
    "                       #'FOG INDEX'\n",
    "                       #'AVG NUMBER OF WORDS PER SENTENCE'\n",
    "                       #'COMPLEX WORD COUNT'\n",
    "                       #'WORD COUNT'\n",
    "                       #'SYLLABLE PER WORD'\n",
    "                        #'PERSONAL PRONOUNS'\n",
    "                       #'AVG WORD LENGTH'\n",
    "#the for loop at the end of this code block allows the data in every link in \"test.URL\" to be extracted and saved to a new csv file called log1.csv                   \n",
    "\n",
    "def easy(link):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import numpy as np\n",
    "    import syllables\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import re\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "    }\n",
    "    r = requests.get(link, headers=headers)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    article_content = soup.find(\"div\", class_=\"td-post-content\").text\n",
    "    diction= pd.read_csv('diction.csv')\n",
    "\n",
    "    punc = '''!()-[]{};:'\"\\,<>/?@#$%^&*_~'''\n",
    "\n",
    "    n_word = diction.loc[diction['Negative'] != 0]\n",
    "    p_word = diction.loc[diction['Positive'] != 0]\n",
    "    pw = p_word.Word.to_string(index=False).replace('\\n', ' ')\n",
    "    nw = n_word.Word.to_string(index=False).replace('\\n', ' ')\n",
    "\n",
    "    positive_word=pw.split()\n",
    "    negative_word=nw.split()\n",
    "\n",
    "    n_word = diction.loc[diction['Negative'] != 0]\n",
    "    p_word = diction.loc[diction['Positive'] != 0]\n",
    "\n",
    "    text = article_content\n",
    "    # Convert text to lowercase and split to a list of words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stop words\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]\n",
    "\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    for word in positive_word:\n",
    "        if word in article_content.upper():\n",
    "            positive_score += 1\n",
    "        \n",
    "    for word in negative_word:\n",
    "        if word in article_content.upper():\n",
    "            negative_score += 1 \n",
    "        \n",
    "    complex_count=0\n",
    "    for i in article_content.split():\n",
    "        if syllables.estimate(i) >=2 :\n",
    "            complex_count+=1\n",
    "        \n",
    "    for e in article_content:\n",
    "        if e in punc:\n",
    "            article_content = article_content.replace(e, \"\")\n",
    "              \n",
    "    polarity_score = (positive_score - negative_score)/ ((negative_score + negative_score) + 0.000001)\n",
    "\n",
    "    word_count = len(tokens_wo_stopwords)\n",
    "\n",
    "    subjectivity_score = (positive_score + negative_score)/ ((word_count) + 0.000001)\n",
    "\n",
    "    number_sentences = len(re.split(r'[.!?]+', article_content))\n",
    "\n",
    "    average_sentence_length = word_count / number_sentences \n",
    "\n",
    "    perc_complex_words = complex_count / word_count\n",
    "\n",
    "    fog_index = 0.4 * (average_sentence_length + perc_complex_words)\n",
    "\n",
    "    avg_word_sentence = word_count / number_sentences\n",
    "\n",
    "    syllable_word = syllables.estimate(article_content)\n",
    "\n",
    "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "    pronouns = len(pronounRegex.findall(article_content))\n",
    "\n",
    "    words = article_content.split()\n",
    "    avg_word_len = sum(len(word) for word in words) / len(words)\\\n",
    "\n",
    "    df2 = pd.DataFrame({'URL':[link],\n",
    "                        'POSITIVE SCORE': [positive_score],\n",
    "                        'NEGATIVE SCORE' : [negative_score],\n",
    "                        'POLARITY SCORE': [polarity_score],\n",
    "                       'SUBJECTIVITY SCORE': [subjectivity_score],\n",
    "                       'AVG SENTENCE LENGTH': [average_sentence_length],\n",
    "                       'PERCENTAGE OF COMPLEX WORDS': [perc_complex_words],\n",
    "                       'FOG INDEX': [fog_index],\n",
    "                       'AVG NUMBER OF WORDS PER SENTENCE': [avg_word_sentence],\n",
    "                       'COMPLEX WORD COUNT': [complex_count],\n",
    "                       'WORD COUNT': [word_count],\n",
    "                       'SYLLABLE PER WORD':[syllable_word],\n",
    "                        'PERSONAL PRONOUNS':[pronouns],\n",
    "                       'AVG WORD LENGTH': [avg_word_len]})\n",
    "    df = seee.append(df2, ignore_index = True)\n",
    "    df.to_csv('log1.csv', mode='a', index=False, header=False)\n",
    "    \n",
    "        \n",
    "for i in test.URL:\n",
    "    easy(i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
